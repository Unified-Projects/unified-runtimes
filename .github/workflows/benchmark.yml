name: Benchmark

on:
    pull_request:
        branches: ["main"]
        paths-ignore:
            - "**/*.md"
            - "LICENCE"
            - ".vscode/**"
            - ".editorconfig"
    workflow_dispatch:
        inputs:
            duration:
                description: "Benchmark duration in seconds"
                required: false
                default: "30"
            runtime_image:
                description: "Runtime image to test with"
                required: false
                default: "openruntimes/node:v5-22"
            function:
                description: "Function type (node, nextjs)"
                required: false
                default: "node"

concurrency:
    group: ${{ github.workflow }}-${{ github.ref }}
    cancel-in-progress: true

env:
    CARGO_TERM_COLOR: always
    BENCH_SECRET: benchmark-secret
    BENCH_DURATION: ${{ github.event.inputs.duration || '30' }}
    BENCH_RUNTIME_IMAGE: ${{ github.event.inputs.runtime_image || 'openruntimes/node:v5-22' }}
    BENCH_FUNCTION: ${{ github.event.inputs.function || 'node' }}
    BENCH_OUTPUT_JSON: benchmark_results.json
    COMPOSE_PROJECT_NAME: urt-bench

jobs:
    benchmark:
        name: Performance Benchmark
        runs-on: ubuntu-latest
        permissions:
            contents: read
        steps:
            - name: Checkout repository
              uses: actions/checkout@8e8c483db84b4bee98b60c0593521ed34d9990e8 # v4

            - name: Set up Docker Buildx
              uses: docker/setup-buildx-action@8d2750c68a42422c14e847fe6c8ac0403b4cbd6f # v3

            - name: Pre-build urt-executor image (cached)
              uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83 # v6
              with:
                  context: .
                  file: docker/Dockerfile
                  tags: ${{ env.COMPOSE_PROJECT_NAME }}-urt-executor:latest
                  load: true
                  cache-from: type=gha
                  cache-to: type=gha,mode=max

            - name: Pre-build bench-runner image (cached)
              uses: docker/build-push-action@263435318d21b8e681c14492fe198d362a7d2c83 # v6
              with:
                  context: .
                  file: crates/urt-executor/Dockerfile.test
                  tags: ${{ env.COMPOSE_PROJECT_NAME }}-bench-runner:latest
                  load: true
                  cache-from: type=gha
                  cache-to: type=gha,mode=max

            - name: Pre-clean stale containers
              run: |
                  docker compose -p "${{ env.COMPOSE_PROJECT_NAME }}" -f crates/urt-executor/docker-compose.test.yml \
                      --profile test --profile bench down -v --remove-orphans 2>/dev/null || true
                  docker ps -a --filter "label=urt.managed" -q | xargs -r docker rm -f 2>/dev/null || true
                  docker network rm e2e-test-network 2>/dev/null || true

            - name: Run dockerized benchmarks
              run: |
                  docker compose -p "${{ env.COMPOSE_PROJECT_NAME }}" -f crates/urt-executor/docker-compose.test.yml \
                      --profile bench up --no-build --abort-on-container-exit --exit-code-from bench-runner

            - name: Capture benchmark output
              if: always()
              run: |
                  docker compose -p "${{ env.COMPOSE_PROJECT_NAME }}" -f crates/urt-executor/docker-compose.test.yml \
                      logs --no-color bench-runner > benchmark_output.txt || true

                  BENCH_CONTAINER_ID=$(docker compose -p "${{ env.COMPOSE_PROJECT_NAME }}" -f crates/urt-executor/docker-compose.test.yml ps -q bench-runner)
                  if [ -n "$BENCH_CONTAINER_ID" ]; then
                      docker cp "$BENCH_CONTAINER_ID:/app/${BENCH_OUTPUT_JSON}" "${BENCH_OUTPUT_JSON}" || true
                  fi

            - name: Write benchmark summary
              if: always()
              run: |
                  {
                      echo "## Benchmark Summary"
                      echo ""

                      if [ -f benchmark_results.json ]; then
                          echo "| Benchmark | RPS | Success % | p50 (ms) | p90 (ms) | p99 (ms) |"
                          echo "|-----------|-----|-----------|----------|----------|----------|"
                          jq -r '
                              def success: if .total_requests == 0 then 0 else (.successful_requests / .total_requests * 100) end;
                              def row($name):
                                  if . == null then empty
                                  else "| \($name) | \(.rps|sprintf(\"%.2f\")) | \(success|sprintf(\"%.2f\"))% | \(.latency.p50_ms|sprintf(\"%.2f\")) | \(.latency.p90_ms|sprintf(\"%.2f\")) | \(.latency.p99_ms|sprintf(\"%.2f\")) |"
                                  end;
                              (.urt // {}) as $u |
                              [
                                  $u.ping | row("Ping Endpoint"),
                                  $u.health | row("Health Endpoint"),
                                  $u.execution | row("Function Execution"),
                                  $u.list_runtimes | row("List Runtimes")
                              ] | .[]
                          ' benchmark_results.json
                          echo ""
                      else
                          echo "_No benchmark_results.json found._"
                          echo ""
                      fi

                      if [ -f benchmark_output.txt ]; then
                          awk '
                              /^## URT Concurrency Scaling/ {printing=1}
                              printing {print}
                              /^\\| Concurrency \\|/ {table=1}
                              table && /^$/ {exit}
                          ' benchmark_output.txt
                          echo ""
                      fi
                  } >> "$GITHUB_STEP_SUMMARY"

            - name: Upload results
              if: always()
              uses: actions/upload-artifact@b7c566a772e6b6bfb58ed0dc250532a479d7789f # v4
              with:
                  name: benchmark-results
                  path: |
                      benchmark_output.txt
                      benchmark_results.json
                  retention-days: 90

            - name: Show executor logs on failure
              if: failure()
              run: |
                  echo "=== URT Executor Logs ==="
                  docker compose -p "${{ env.COMPOSE_PROJECT_NAME }}" -f crates/urt-executor/docker-compose.test.yml \
                      logs --no-color urt-executor | tail -100

            - name: Clean up resources
              if: always()
              run: |
                  docker compose -p "${{ env.COMPOSE_PROJECT_NAME }}" -f crates/urt-executor/docker-compose.test.yml down -v --remove-orphans 2>/dev/null || true
                  docker ps -a --filter "label=urt.managed" -q | xargs -r docker rm -f 2>/dev/null || true
                  docker network rm e2e-test-network 2>/dev/null || true
                  docker system prune -f 2>/dev/null || true
                  docker volume prune -f 2>/dev/null || true
                  rm -f benchmark_output.txt benchmark_results.json 2>/dev/null || true
